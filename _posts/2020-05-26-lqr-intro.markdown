---
layout: post
title:  "Introduction to LQR from RL perspective"
author: Vincent Zhang
date:   2020-05-26
categories: research rl control
usemathjax: true
---


This is an introduction to Linear Quadratic Regulator (LQR) for people with an RL background.  
I view LQR as an important class of control problems that can be solved with RL algorithms.

There are a lot of excellent tutorials online about LQR. But they are mostly in line with the control textbooks.
I have not found any that is geared toward RL researchers/practitioners. 
So here you go, I will try to explain LQR using the RL terminology.

## Motivation
Many real-world control tasks can be modeled as LQR, which has been extensively studied by the control theory community. In the RL community, LQR seems less popular, although it has been argued by Ben Becht that attempting to solve this (easy?) problem using RL would help us to better understand the capability and limitation of RL algorithms [2].
I will refer the readers to his blogs instead of repeating why that's the case.

<!-- **Environments related to RL community** -->
What I want to point out here is that, the RL community may have been throwing RL algorithms at LQR problems without necessarily knowing it.
If we take a close look at the simulated control tasks commonly used in RL research today, lots of them can be approximated as LQR problems. For instance, some control environments in Openai Gym like pendulum, reacher and FetchReach, are approximately LQR. The `approximation` is two folds: first, it's in the sense that the system dynamics is often nonlinear that can be `approximated` by a linear model; second, the original reward function does not exactly follow the LQR formulation but is very close.

I'd like to think of LQR as 
```
a counterpart of tabular MDP 
in continuous-time with continuous state and action space.
```

## What is LQR?
Let us first get some terminology straight.
LQR stands for Linear Quadratic Regulator.

- `Linear` refers to the linear system dynamics.
- `Quadratic` refers to the quadratic cost.
- `Regulator` (I guess) historically refers to `regulating` some signal of a system.

In total, there are four sub-types of LQR that we consider:

- Discrete-time Deterministic LQR
- Discrete-time Stochastic LQR
- Continuous-time Deterministic LQR
- Continuous-time Stochastic LQR [1]

The key difference between the deterministic versions and the stochastic versions is that the former is noiseless whereas the latter considers noise in the dynamics.
The stochastic version is what we will truly encounter in the real world. 
Nonetheless, we will first go over the deterministic setup since it is easier to understand and analyze.

## Discrete-time Deterministic LQR

Discrete-time Deterministic LQR can be expressed as:

$$x_{k+1} = A x_k + B u_k$$

$$c_{k} = x_k^\top S x_k + u_k^\top R u_k$$

where $$x_k\in \mathcal{R}^n$$ and $$u_k\in \mathcal{R}^m$$ denote the state and action at step $$k$$ respectively (as opposed to $$s_k, a_k$$).
$$A$$ and $$B$$ are matrices of dimensions of `n x n` and `n x m`.
$$c_k$$ denotes the cost at step $$k$$.
$$S$$ and $$R$$ are matrices of dimensions of `n x n` and `m x m`, where $$S$$ is positive semi-definite and $$R$$ is positive definite: $$S\succeq 0$$ and $$R\succ 0$$.

Instead of choosing actions to maximize the sum of rewards $$G_t$$ as in RL, 
in LQR the objective is to find a sequence of control that **minimizes the sum of cost**.

<!-- Discounted vs Average Reward Formulation -->
There are finite horizon and infinite horizon settings, corresponding to episodic and continuing tasks in RL. And just like in RL, we can formulate the objective as either discounted or average.

**Discounted Cost**:

$$ J(u_0, u_1, ...) = \sum_{k=0}^{N} \gamma^{k} \mathbb{E}[c_k] = \sum_{k=0}^{N} \gamma^{k} \mathbb{E}(x_k^\top S x_k + u_k^\top R u_k) $$

where the discount factor $$\gamma \in [0,1]$$.

**Average Cost**:

$$ J(u_0, u_1, ...) = \lim_{N\to \infty} \frac{1}{N} \sum_{k=0}^{N} \mathbb{E}[c_k] = \lim_{N\to \infty} \frac{1}{N} \sum_{k=0}^{N} \mathbb{E}(x_k^\top S x_k + u_k^\top R u_k) $$

In both objectives, the expectation is wrt the noise so it's not necessary for the deterministic case but we leave it there for completeness.

**Some properties of LQR**
What's nice about LQR is that, its structure allows for some neat properties:
```
* the state and action value function are a quadratic function
* the optimal policy (called optimal control in LQR), is linear in the state.
* if we know the parameters of the problem, the optimal policy, 
  and the value functions for any given linear policies can be computed
  using routines in python, Julia, Matlab etc.
```

**Optimal Policy $$\pi_*$$**

The optimal control is given by

\begin{equation}
u_* = K_* x = -\gamma (R  + \gamma B^\top P_* B)^{-1} B^\top P_* A x 
\label{eq:ustar}
\end{equation}

where $$x$$ is any given state and the $$P_*$$ is the solution to Algebraic Riccati Equation (ARE):

$$ P = S + \gamma A^\top P A - \gamma^2 A^\top P B(R+\gamma B^\top P B)^{-1} B^\top P A $$

$$P$$ is `n x n` symmetric matrix. 
Note that with the discount factor, the equation above is not a typical Riccati Equation as you might find on wikipedia but it's easy to show that it is equivalent to a typical ARE with a discounted A and B matrix.

Let's take a moment to understand what it implies.
- In RL, to find the optimal policy, if the dynamics $$ p(s_{k+1}, r|s_k,a_k) $$ is known, we would need to run some iterative algorithms like value or policy iteration. 
But in LQR, the optimal policy can be computed directly.
<!-- or monte-carlo, bootstrapping or policy gradient if dynamics is unknown. -->
- This matrix $$P_*$$ is the steady-state of a $$P_k$$ that evolves backward in time. 
  In other words, we can only hope to achieve $$P_*$$ if the time is sufficiently away from the terminal time.
  In finite-horizon, typically we don't attain $$P_*$$ so the optimal control $$u_*$$ in Eq. \eqref{eq:ustar} is in fact sub-optimal.

**State Value Function $$V_\pi(x)$$**

The value function can be expressed as 

\begin{equation}
V_\pi(x_k) = x_k^\top P_\pi x_k
\end{equation}

where $$P_\pi$$ is a cost matrix associated with the policy $$\pi: \mathcal{X} \to \mathcal{U}$$ that maps the state space $$\mathcal{X}$$ to action space $$\mathcal{U}$$. 
Since the optimal policy is contained in the familiy of linear policies, we can assume that the policy is always linear: $$\pi(x_k) = K x_k$$ where $$K \in \mathcal{R}^{m\times n} $$.

Similar to cost matrix $$P_*$$ for the optimal policy discussed above, the cost matrix $$P_\pi$$ for an arbitrary policy $$\pi = K_\pi x$$ also evolves backward in time and has a steady state.
But instead of using ARE to solve it, we should use Lyapunov Equation:

\begin{equation}

\end{equation}



**Action Value Function $$Q_\pi(x, u)$$**

It can be shown that the action value function is quadratic in state and action [4].
$$
\begin{align}
\nonumber Q_\pi(x, u) &=  c(x,u) + \gamma  V_\pi( Ax + Bu) \\ 
&= [x\ u]^\top \begin{bmatrix} S + \gamma A^\top P_\pi A &  \gamma A^\top P_\pi B \\ \gamma B^\top P_\pi A &  R+\gamma B^\top P_\pi B \end{bmatrix} [x\ u] \\
&= [x\ u]^\top \begin{bmatrix} H_{11} &  H_{12} \\ H_{21} &  H_{22} \end{bmatrix} [x\ u] 
\label{eq:Q}
\end{align}
$$

where $$H$$ is a positive definite matrix of size `(n+m) x (n+m)`.
Combined with Eq.\eqref{eq:ustar}, the greedy action at state $$x$$ wrt the current policy $$\pi$$ is:

\begin{equation}
u_g = -H_{22}^{-1} H_{21}
\end{equation}

What this entails is that,
if we know the matrix $$H$$, we can perform policy improvement by taking the greedy action $$u_g$$.


**Concept**
Riccati Equation

Lyapunov Equation

Stability, Controllability

Function Approximation










## Reference:
[1] Wendell, H. FLEMING, R. W. Rishel, and W. Raymond. "Deterministic and stochastic optimal control." (1975).

[2] Blog of Ben Recht: http://www.argmin.net/2018/02/08/lqr/

[3] D. P. Bertsekas.Dynamic programming and optimal control,  volume 1.  Athena scientific, Belmont, MA, 2005.

[4] S. J. Bradtke.  Reinforcement learning applied to linear quadratic regulation. In Advances in neural information processing systems, pages 295â€“302, 1993

<!-- [2] Stephen's Lecture Notes

[4] Csaba's paper

[5] Ben's paper -->


