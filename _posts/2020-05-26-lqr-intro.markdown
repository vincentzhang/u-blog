---
layout: post
title:  "An Introduction to LQR from RL perspective"
author: Vincent Zhang
date:   2020-05-26
categories: research rl control
usemathjax: true
---


This is an introduction to Linear Quadratic Regulator (LQR) for people with an RL background.  
I view LQR as an important class of control problems that can be solved with RL algorithms.

There are a lot of excellent tutorials online about LQR. But they are mostly in line with the control theory textbooks.
I have not found any that is geared toward RL researchers/practitioners. 
So here you go, I will try to explain LQR using the RL terminology in this post.
It's intended to cover the concepts that can be related to the RL side of things, rather than a comprehensive coverage of all details of LQR.

## Motivation
Many real-world control tasks can be modeled as LQR, which has been extensively studied by the control theory community. In the RL community, LQR seems less popular, although it has been argued by Ben Becht that attempting to solve this (easy?) problem using RL would help us to better understand the capability and limitation of RL algorithms [2].
I will refer the readers to his blogs instead of repeating why that's the case.

<!-- **Environments related to RL community** -->
What I want to point out here is that, the RL community may have been throwing RL algorithms at LQR problems without necessarily knowing it.
If we take a close look at the simulated control tasks commonly used in RL research today, lots of them can be approximated as LQR problems. For instance, some control environments in Openai Gym like pendulum, reacher and FetchReach, are approximately LQR. The `approximation` is two folds: first, it's in the sense that the system dynamics is often nonlinear that can be `approximated` by a linear model; second, the original reward function does not exactly follow the LQR formulation but is very close.

I'd like to think of LQR as 
```
a counterpart of tabular MDP 
in continuous-time with continuous state and action space.
```

## What is LQR?
Let us first get some terminology straight.
LQR stands for Linear Quadratic Regulator.

- `Linear` refers to the linear system dynamics.
- `Quadratic` refers to the quadratic cost.
- `Regulator` (I guess) historically refers to `regulating` some signal of a system.

In total, there are four sub-types of LQR that we consider:

- Discrete-time Deterministic LQR
- Discrete-time Stochastic LQR
- Continuous-time Deterministic LQR
- Continuous-time Stochastic LQR

The key difference between the deterministic versions and the stochastic versions is that the former is noiseless whereas the latter considers noise in the dynamics.
The stochastic version is what we will truly encounter in the real world. 
Nonetheless, we will first go over the deterministic setup since it is easier to understand and analyze.

## Discrete-time Deterministic LQR

Discrete-time Deterministic LQR can be expressed as:

$$x_{k+1} = A x_k + B u_k$$

$$c_{k} = x_k^\top S x_k + u_k^\top R u_k$$

where $$x_k\in \mathcal{R}^n$$ and $$u_k\in \mathcal{R}^m$$ denote the state and action at step $$k$$ respectively (as opposed to $$s_k, a_k$$).
$$A$$ and $$B$$ are matrices of dimensions of `n x n` and `n x m`.
$$c_k$$ denotes the cost at step $$k$$.
$$S$$ and $$R$$ are matrices of dimensions of `n x n` and `m x m`, where $$S$$ is positive semi-definite and $$R$$ is positive definite: $$S\succeq 0$$ and $$R\succ 0$$.

Instead of choosing actions to maximize the sum of rewards $$G_t$$ as in RL, 
in LQR the objective is to find a sequence of control that **minimizes the sum of cost**.

<!-- Discounted vs Average Reward Formulation -->
There are finite horizon and infinite horizon settings, corresponding to episodic and continuing tasks in RL. And just like in RL, we can formulate the objective as either discounted or average.

**Discounted Cost**:

$$ J(u_0, u_1, ...) = \sum_{k=0}^{N} \gamma^{k} \mathbb{E}[c_k] = \sum_{k=0}^{N} \gamma^{k} \mathbb{E}(x_k^\top S x_k + u_k^\top R u_k) $$

where the discount factor $$\gamma \in [0,1]$$.

**Average Cost**:

$$ J(u_0, u_1, ...) = \lim_{N\to \infty} \frac{1}{N} \sum_{k=0}^{N} \mathbb{E}[c_k] = \lim_{N\to \infty} \frac{1}{N} \sum_{k=0}^{N} \mathbb{E}(x_k^\top S x_k + u_k^\top R u_k) $$

In both objectives, the expectation is wrt the noise so it's not necessary for the deterministic case but we leave it there for completeness.

**Some properties of LQR**

What's nice about LQR is that, its structure allows for some neat properties:
```
* the state and action value function are a quadratic function
* the optimal policy (called optimal control in LQR), is linear in the state.
* if we know the parameters of the problem, the optimal policy, 
  and the value functions for any given linear policies can be computed
  using routines in python, Julia, Matlab etc.
```

**Optimal Policy $$\pi_*$$**

The optimal control is given by

\begin{equation}
u_* = K_* x = -\gamma (R  + \gamma B^\top P_* B)^{-1} B^\top P_* A x 
\label{eq:ustar}
\end{equation}

where $$x$$ is any given state and the $$P_*$$ is the solution to Algebraic Riccati Equation (ARE):

$$ P = S + \gamma A^\top P A - \gamma^2 A^\top P B(R+\gamma B^\top P B)^{-1} B^\top P A $$

$$P$$ is `n x n` symmetric matrix. 
Note that with the discount factor, the equation above is not a typical Riccati Equation as you might find on wikipedia but it's easy to show that it is equivalent to a typical ARE with a discounted A and B matrix.

Let's take a moment to understand what it implies.
- In RL, to find the optimal policy, if the dynamics $$ p(s_{k+1}, r|s_k,a_k) $$ is known, we would need to run some iterative algorithms like value or policy iteration. 
But in LQR, the optimal policy can be computed directly.
<!-- or monte-carlo, bootstrapping or policy gradient if dynamics is unknown. -->
- This matrix $$P_*$$ is the steady-state of a $$P_k$$ that evolves backward in time. 
  In other words, we can only hope to achieve $$P_*$$ if the time is sufficiently away from the terminal time.
  In finite-horizon, typically we don't attain $$P_*$$ so the optimal control $$u_*$$ in Eq. \eqref{eq:ustar} is in fact sub-optimal.

**State Value Function $$V_\pi(x)$$**

The value function can be expressed as 

\begin{equation}
V_\pi(x_k) = x_k^\top P_\pi x_k
\end{equation}

where $$P_\pi$$ is a cost matrix associated with the policy $$\pi: \mathcal{X} \to \mathcal{U}$$ that maps the state space $$\mathcal{X}$$ to action space $$\mathcal{U}$$. 
Since the optimal policy is contained in the familiy of linear policies, we can assume that the policy is always linear: $$\pi(x_k) = K_\pi x_k$$ where $$K_\pi \in \mathcal{R}^{m\times n} $$.

Similar to cost matrix $$P_*$$ for the optimal policy discussed above, the cost matrix $$P_\pi$$ for an arbitrary policy $$\pi = K_\pi x$$ also evolves backward in time and has a steady state.
But instead of using ARE to solve it, we should use a different equation:

\begin{equation}
-P_k + S + K^\top R K + \gamma [(A+BK)^\top P_{k+1}(A+BK)] = 0
\end{equation}

Let $$M = S + K^\top R K$$, $$L = \sqrt{\gamma}(A+BK)$$, 
we can rewrite the equation above as
\begin{equation}
-P_k + M + L^\top P_{k+1} L = 0
\end{equation}

which is called [Lyapunov Equation](https://en.wikipedia.org/wiki/Lyapunov_equation){:target="_blank"}.

$$P_\pi$$ is the steady-state solution to this Lyapunov Equation. 
Note that, if the horizon is finite and not long enough to attain steady-state, we can solve for $$P_\pi$$ recursively (backward in time) where the base case is given by the $$P_k$$ at the terminal step $$N$$ where we assume no control cost: $$V(N) = x_N ^\top S x_N$$.

**Action Value Function $$Q_\pi(x, u)$$**

It can be shown that the action value function is quadratic in state and action [4].
$$
\begin{align}
\nonumber Q_\pi(x, u) &=  c(x,u) + \gamma  V_\pi( Ax + Bu) \\ 
&= [x\ u]^\top \begin{bmatrix} S + \gamma A^\top P_\pi A &  \gamma A^\top P_\pi B \\ \gamma B^\top P_\pi A &  R+\gamma B^\top P_\pi B \end{bmatrix} [x\ u] \\
&= [x\ u]^\top \begin{bmatrix} H_{11} &  H_{12} \\ H_{21} &  H_{22} \end{bmatrix} [x\ u] 
\label{eq:Q}
\end{align}
$$

where $$[x\ u]$$ is a column vector from concatenating the vectors $$x$$ and $$u$$. $$H$$ is a positive definite matrix of size `(n+m) x (n+m)`.
Combined with Eq.\eqref{eq:ustar}, the greedy action at state $$x$$ wrt the current policy $$\pi$$ is:

\begin{equation}
u_g = -H_{22}^{-1} H_{21}
\label{eq:greedy}
\end{equation}

What this implies is that,
if we know the matrix $$H$$, we can perform policy improvement by taking the greedy action $$u_g$$.
Now this is one of the nicest feature of LQR from RL perspective.
It is well known that problems involving continuous actions are difficult for RL algorithms. But in the case of LQR, we can compute the greedy policy if we have some estimate of matrix $$H$$ after policy evaluation, just like in discrete action case!

**Model-based vs Model-free, and Function Approximation**

In most cases, we don't know the true parameters $$A, B$$ of the system.
Model-based algorithms estimate the dynamic model and compute the optimal control based on the estimated parameters $$A, B$$.
Model-free algorithms do not explicitly estimate the model. Instead, we approximate the optimal action value function by estimating the matrix $$H$$ in Eq.\eqref{eq:Q} and compute the optimal control by Eq.\eqref{eq:greedy}.

What functions shall we choose for approximating the action value function?
We can throw a neural network at it. But that might be an overkill. 
Since we know that the action value funciton is quadratic, we can build the quadratic basis functions from the state-action pair $$[x\ u]$$ and approximate the action value as a linear combination of them:

 $$Q_\pi(x, u) = \theta^\top \phi(x, u) $$ 
 
 where $$\theta$$ is the weight vector and the $$ \phi(x, u) $$ is the quadratic basis feature vector.
 There are $$(n+m)^2$$ entries in the matrix $$H$$ but we do not need to estimate that many parameters due to $$H$$ being symmetric. 

 The dimensionality of $$\theta$$ and $$ \phi(x, u) $$ is `(n+m+1)(n+m)/2`.

**Stability, Controllability**

<!-- Optimal control theory offers saftey guarantees.  -->
Two quite useful concepts from control theory are stability and controllability [3].

`Stability` of policy $$K_\pi$$ tells us if following this policy would lead to divergence.
It is determined by $$A, B$$ and $$K_\pi$$. For a policy to be stable, the eigenvalues of the matrix $$(A+BK_\pi)$$ needs to be strictly within the unit circle(often complex numbers).

`Controllability` of the system tells us for any initial state $$x_0$$, if there exists some policy that drives the state to zero. It is determined by $$A, B$$.
For a system to be controllable, the matrix $$[B | AB | A^2B | ... | A^{k}B | ... | A^{n−1}B ]$$ needs to be full rank.

## Discrete-time Stochastic LQR

When we consider the noise during the transition of the system, the dynamic can be written as

$$x_{k+1} = A x_k + B u_k + w_k$$

where $$w_k \in \mathcal{R}^n $$ is the noise term, often modeled as an i.i.d multi-variate Gaussian $$w_k \sim \mathbb{N}(0,\Sigma_w) $$.

What's nice about it is that despite the noise, the optimal policy $$\pi_*$$ and the matrices $P_*, P_\pi$ remain the same as the deterministic case. The value and action value functions differ from the ones in deterministic cases by only a constant.

## Continuous-time Deterministic LQR
...

## Continuous-time Stochastic LQR [1]
...









## Reference:
[1] Wendell, H. FLEMING, R. W. Rishel, and W. Raymond. "Deterministic and stochastic optimal control." (1975).

[2] Blog of Ben Recht: http://www.argmin.net/2018/02/08/lqr/

[3] D. P. Bertsekas.Dynamic programming and optimal control,  volume 1.  Athena scientific, Belmont, MA, 2005.

[4] S. J. Bradtke.  Reinforcement learning applied to linear quadratic regulation. In Advances in neural information processing systems, pages 295–302, 1993

[5] Stephen Boyd's EE363 Lecture Slides

<!-- [4] Csaba's paper

[5] Ben's paper --> 


